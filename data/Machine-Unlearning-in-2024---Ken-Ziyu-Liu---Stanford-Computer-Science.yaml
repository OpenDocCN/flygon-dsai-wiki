- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-05-07 15:40:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-05-07 15:40:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Machine Unlearning in 2024 - Ken Ziyu Liu - Stanford Computer Science
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024年机器反学习 - 刘子昱 - 斯坦福计算机科学
- en: 来源：[https://ai.stanford.edu/~kzliu/blog/unlearning](https://ai.stanford.edu/~kzliu/blog/unlearning)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ai.stanford.edu/~kzliu/blog/unlearning](https://ai.stanford.edu/~kzliu/blog/unlearning)
- en: Written by [Ken Liu](https://ai.stanford.edu/~kzliu) ∙ May 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Ken Liu](https://ai.stanford.edu/~kzliu) ∙ May 2024 撰写
- en: ▸ Table of Contents
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ▸ 目录
- en: 1\. A bit of history & motivations for unlearning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1\. 反学习的历史和动机
- en: 2\. Forms of unlearning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2\. 反学习的形式
- en: 2.1\. Exact unlearning
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.1\. 精确反学习
- en: 2.2\. "Unlearning" via differential privacy
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.2\. 通过差分隐私进行“反学习”
- en: 2.3\. Empirical unlearning with known example space
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.3\. 已知示例空间下的经验性反学习
- en: 2.4\. Empirical unlearning with unknown example space
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.4\. 未知示例空间下的经验性反学习
- en: 2.5\. Just ask for unlearning?
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.5\. 只需要求反学习吗？
- en: 3\. Evaluating unlearning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3\. 评估反学习
- en: 4\. Practice, pitfalls, and prospects of unlearning
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4\. 反学习的实践、陷阱和前景
- en: 4.1\. The spectrum of unlearning hardness
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4.1\. 反学习难度的谱系
- en: 4.2\. Copyright protection
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4.2\. 版权保护
- en: 4.3\. Retrieval-based AI systems
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4.3\. 基于检索的AI系统
- en: 4.4\. AI safety
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4.4\. AI安全
- en: '* * *'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As our ML models today become larger and their (pre-)training sets grow to inscrutable
    sizes, [people](https://unlearning-challenge.github.io/) [are](https://www.axios.com/newsletters/axios-ai-plus-117900d2-3a2c-4741-b807-bb8f2f9bb035.html?chunk=0&utm_term=twsocialshare#story0)
    [increasingly](https://www.safe.ai/blog/wmdp-benchmark) [interested](https://hbswk.hbs.edu/item/qa-seth-neel-on-machine-unlearning-and-the-right-to-be-forgotten)
    in the concept of **machine unlearning** to edit away undesired things like private
    data, stale knowledge, copyrighted materials, toxic/unsafe content, dangerous
    capabilities, and misinformation, without retraining models from scratch.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们今天的机器学习模型变得越来越庞大，它们的（预）训练集也变得难以理解的庞大，[人们](https://unlearning-challenge.github.io/)
    [对](https://www.axios.com/newsletters/axios-ai-plus-117900d2-3a2c-4741-b807-bb8f2f9bb035.html?chunk=0&utm_term=twsocialshare#story0)
    [对](https://www.safe.ai/blog/wmdp-benchmark) [机器反学习](https://hbswk.hbs.edu/item/qa-seth-neel-on-machine-unlearning-and-the-right-to-be-forgotten)
    概念越来越感兴趣，以编辑掉不需要的东西，如私人数据、陈旧知识、受版权保护的材料、有毒/不安全的内容、危险能力和错误信息，而无需从头开始重新训练模型。
- en: Machine unlearning can be broadly described as removing the influences of training
    data from a trained model. At its core, unlearning on a *target model* seeks to
    produce an *unlearned model* that is equivalent to—or at least “behaves like”—a
    *retrained model* that is trained on the same data of target model, minus the
    information to be unlearned.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器反学习可以被广泛地描述为从已训练模型中移除训练数据的影响。在本质上，对于一个 *目标模型* 的反学习旨在产生一个等同于——或至少“行为像”—使用目标模型相同的数据训练的
    *重新训练模型*，但减去了要反学习的信息。
- en: There’s a lot hidden in the above description. How do we describe the information
    to be unlearned? Do we always have ground-truth retrained models? If not, how
    do we actually evaluate the unlearning? Can we even verify and audit the unlearning?
    Is *pretending* to unlearn, as humans often do, sufficient? Is unlearning even
    the right solution? If so, for what problems?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述中隐藏着很多内容。我们如何描述要反学习的信息？我们是否总是有地面真实的重新训练过的模型？如果没有，我们如何实际评估反学习？我们甚至能验证和审计反学习吗？假装反学习，就像人类经常做的那样，是否足够？反学习甚至是正确的解决方案吗？如果是，针对什么问题？
- en: 'The precise definitions of unlearning, the techniques, the guarantees, and
    the metrics/evaluations would depend on:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 反学习的精确定义、技术、保证以及指标/评估将取决于：
- en: The ML task (e.g., binary classification or language modeling);
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习任务（例如，二元分类或语言建模）；
- en: The data to unlearn (e.g., a set of images, news articles, or the knowledge
    of making [napalm](https://youtu.be/zjkBMFhNj_g?t=2774));
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要反学习的数据（例如，一组图像、新闻文章或制作 [凝固汽油](https://youtu.be/zjkBMFhNj_g?t=2774) 的知识）；
- en: The unlearning algorithm (e.g., heuristic fine-tuning vs deleting model components);
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反学习算法（例如，启发式微调 vs 删除模型组件）；
- en: The goal of unlearning (e.g., for user privacy or harmfulness removal).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反学习的目标（例如，用于用户隐私或有害内容的移除）。
- en: In this educational post, I hope to give a gentle, general ML audience introduction
    to machine unlearning and touch on things like [copyright protection](https://arxiv.org/abs/2303.15715),
    [New York Times v. OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html),
    [right-to-be-forgotten](https://gdpr.eu/right-to-be-forgotten/), [NeurIPS machine
    unlearning challenge](https://unlearning-challenge.github.io/), [retrieval-based
    AI systems](https://arxiv.org/abs/2005.11401), [AI safety](https://www.mlsafety.org/),
    along with some of my thoughts on the field. While unlearning is broad topic applicable
    to most ML models, we will focus a lot on [foundation models](https://en.wikipedia.org/wiki/Foundation_model).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇教育性的文章中，我希望为普通的ML受众介绍机器遗忘，并涉及诸如[版权保护](https://arxiv.org/abs/2303.15715)、[纽约时报诉OpenAI案](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)、[被遗忘的权利](https://gdpr.eu/right-to-be-forgotten/)、[NeurIPS机器遗忘挑战](https://unlearning-challenge.github.io/)、[基于检索的AI系统](https://arxiv.org/abs/2005.11401)、[AI安全](https://www.mlsafety.org/)，以及我对这一领域的一些想法。虽然遗忘是一个广泛适用于大多数ML模型的广泛主题，但我们将重点关注[基础模型](https://en.wikipedia.org/wiki/Foundation_model)。
- en: 1\. A bit of history & motivations for unlearning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 关于遗忘的一点历史和动机
- en: People have thought about the unlearning problem [for](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)
    [a while](https://proceedings.neurips.cc/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html)
    [now](https://arxiv.org/abs/1912.03817). The initial research explorations were
    primarily driven by [Article 17](https://gdpr.eu/article-17-right-to-be-forgotten/)
    of GDPR (European Union’s privacy regulation), often referred to as “[right-to-be-forgotten](https://gdpr.eu/right-to-be-forgotten/)”
    (**RTBF**) since 2014\. RTBF basically says a user has the right to request deletion
    of their data from a service provider (e.g. deleting your Gmail account).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人们已经思考了遗忘问题[一段时间](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)
    [现在](https://proceedings.neurips.cc/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html)。最初的研究探索主要是由GDPR（欧盟的隐私法规）的[第17条](https://gdpr.eu/article-17-right-to-be-forgotten/)驱动的，通常被称为“[被遗忘的权利](https://gdpr.eu/right-to-be-forgotten/)”（**RTBF**）自2014年以来。RTBF基本上表示用户有权要求从服务提供商（例如删除您的Gmail帐户）那里删除他们的数据。
- en: RTBF was well-intentioned. It was also very actionable when said service providers
    store user data in a structured way, like how Google [removed](https://www.cnbc.com/2014/10/13/google-removes-170000-right-to-be-forgotten-links.html)
    a bunch of links from its index in repsonse to RTBF requests.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RTBF的初衷是好的。当服务提供商以结构化方式存储用户数据时，如谷歌以[回应](https://www.cnbc.com/2014/10/13/google-removes-170000-right-to-be-forgotten-links.html)RTBF请求从其索引中删除了一堆链接时，它也是非常可行的。
- en: However, RTBF wasn’t really proposed with machine learning in mind. In 2014,
    policymakers wouldn’t have predicted that deep learning will be a giant hodgepodge
    of data & compute, and that separating and interpreting this hodgepodge turned
    out to be hard. The hardness of erasing data from ML models has subsequently motivated
    research on what is later referred to as “[data deletion](https://proceedings.neurips.cc/paper_files/paper/2019/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf)”
    and “[machine](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)
    [unlearning](https://arxiv.org/pdf/1912.03817.pdf)”.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RTBF并不是真正以机器学习为目标提出的。在2014年，决策者们不会预测到深度学习将成为数据和计算的巨大混合物，并且分离和解释这个混合物是困难的。从ML模型中删除数据的难度随后促使了所谓的“[数据删除](https://proceedings.neurips.cc/paper_files/paper/2019/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf)”和“[机器](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)
    [遗忘](https://arxiv.org/pdf/1912.03817.pdf)”的研究。
- en: '**A decade later in 2024, user privacy is no longer the only motivation for
    unlearning.** We’ve gone from training small convolutional nets on face images
    to training giant language models on pay-walled, [copyrighted](https://www.theverge.com/23961021/ai-art-copyright-training-ownership-fair-use),
    [toxic](https://support.google.com/youtube/answer/6000976?hl=en), dangerous, and
    otherwise harmful content, all of which we may want to “erase” from the ML models—sometimes
    with access to only a handful of examples. The nature of the models has changed
    too. Instead of using many small specialized models each good at one task, people
    started using a single [giant model](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    that knows just about any task.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**十年后的2024年，用户隐私不再是取消学习的唯一动机。**我们已经从在面部图像上训练小型卷积网络转变为在有版权、[有毒](https://support.google.com/youtube/answer/6000976?hl=en)、危险以及其他有害内容上训练巨型语言模型，所有这些内容我们都可能希望从机器学习模型中“抹去”——有时只能使用少数示例来训练。模型的性质也发生了变化。人们开始使用一个[巨型模型](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)，它几乎可以处理任何任务，而不是使用许多小型专门化模型，每个模型擅长一项任务。'
- en: 'Currently, I think the motivations for unlearning fall into two categories:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我认为取消学习的动机可以分为两类：
- en: '**Access revocation** (think unlearning private and copyrighted data). In an
    ideal world, data should be thought of as “borrowed” (possibly unpermittedly)
    and thus can be “returned”, and unlearning should enable such revocation.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**访问撤销**（思考取消对私有和受版权保护的数据的学习）。在理想的情况下，数据应该被视为“借用的”（可能是未经许可的），因此可以被“归还”，而取消学习应该使这种撤销成为可能。'
- en: Unlearning is challenging from this perspective. One key difficulty is that
    our limited understanding of deep learning itself makes data trained into a model
    akin to “consumables” (which can’t just be “returned” after consumption). Data
    may also be non-fungible (e.g. your chat history) and may even be thought of as
    [labor](https://www.radicalxchange.org/media/papers/data-freedom-act.pdf) with
    its own financial and control interests. Another challenge is that access revocation
    may require a *proof* of unlearning; as we will explore in the coming sections,
    this isn’t always possible.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这个角度来看，取消学习是具有挑战性的。一个关键困难是我们对深度学习本身的有限理解使得训练到模型中的数据类似于“可消耗品”（消耗后不能简单地“归还”）。数据也可能是不可替代的（例如，你的聊天记录），甚至可以被视为[劳动](https://www.radicalxchange.org/media/papers/data-freedom-act.pdf)，具有自己的财务和控制利益。另一个挑战是访问撤销可能需要取消学习的*证明*；正如我们将在接下来的章节中探讨的，这并不总是可能的。
- en: These difficulties suggest that it’s perhaps also worth revising laws like RTBF
    and thinking about alternatives such as *data markets*, where data owners are
    properly compensated so they won’t want to request unlearning in the first place.
    To illustrate, suppose Bob ate Alice’s cheesecake (data), Alice would much rather
    Bob pay her or return something equivalent (compensation) than Bob puking to his
    pre-eating state (unlearning).
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些困难表明，也许值得重新审视像RTBF这样的法律，并考虑一些替代方案，例如*数据市场*，在那里，数据所有者能够得到适当的补偿，因此他们就不会首先要求取消学习。举例说明，假设Bob吃了Alice的芝士蛋糕（数据），Alice更愿意Bob支付她或归还等价物（补偿），而不是Bob恢复到吃之前的状态（取消学习）。
- en: 'In practice, one way to implement access revocation is via some form of *periodic
    re-training* of the base model. Many model providers already do this to keep their
    models competitive and up-to-date. For example, OpenAI can collect a bunch of
    unlearning requests, and batch-satisfy them during the re-training every year
    (or, guided by RTBF’s “[undue delay](https://gdpr.eu/right-to-be-forgotten/)”
    period by which the request must be satisfied). More broadly, this suggests *socio-technical
    solutions* for unlearning: policymakers can mandate such periodic re-training
    and set economically viable deadlines to offload the costs to the model owners.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在实践中，实现访问撤销的一种方式是通过某种形式的*周期性重新训练*基础模型。许多模型提供者已经在做到这一点，以保持他们的模型具有竞争力和更新。例如，OpenAI
    可以收集一堆取消学习请求，并在每年的重新训练中批量满足它们（或者，根据RTBF规定的“[不当延迟](https://gdpr.eu/right-to-be-forgotten/)”期限来满足请求）。更广泛地说，这提出了取消学习的*社会技术解决方案*：决策者可以强制执行这种周期性重新训练，并设定经济上可行的截止日期，将成本转嫁给模型所有者。
- en: '**Model correction & editing** (think toxicity, bias, stale/dangerous knowledge
    removal). That is, the model was trained on something undesirable and we’d like
    to fix it. This is closely related to the [model editing](https://arxiv.org/abs/2110.11309)
    literature. The concept of “[corrective machine unlearning](https://arxiv.org/abs/2402.14015)”,
    where unlearning serves to correct the impact of bad data, was recently proposed
    to capture this motivation. From this perspective, unlearning may also be viewed
    as a post-training risk mitigation mechanism for AI safety concerns (discussed
    further in Section 4).'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型修正和编辑**（考虑毒性、偏见、过时/危险知识的移除）。也就是说，模型是在一些不良条件下训练的，我们希望修复它。这与[模型编辑](https://arxiv.org/abs/2110.11309)文献密切相关。最近提出了“[纠正型机器遗忘](https://arxiv.org/abs/2402.14015)”的概念，其中遗忘用于纠正不良数据的影响。从这个角度来看，遗忘也可以被视为用于AI安全问题的后训练风险缓解机制（在第
    4 节进一步讨论）。'
- en: Unlike access revocation, we could be more lenient towards with model correction
    since the edit is more of a desire than a necessity mandated by law, much like
    model accuracy on image classification or toxicity of generated text. (Of course,
    these can cause real harm too.) Here, we won’t necessarily need *formal guarantees*
    for the unlearning to be practically useful; we have plenty of examples where
    people would happily deploy models that are deemed “sufficiently safe”. The recent
    [WMDP benchmark](https://www.wmdp.ai/), which quizzes a model on hazardous knowledge,
    is a good example of empirically evaluating unlearning efficacy.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与访问撤销不同，对于模型修正我们可能会更宽容，因为编辑更多地是一种愿望，而不是法律所规定的必要性，就像图像分类的模型准确性或生成文本的毒性一样。 （当然，这些也可能造成真正的伤害。）在这里，我们不一定需要*正式保证*才能使遗忘在实践中有用；我们有很多例子可以证明，人们愿意部署被认为“足够安全”的模型。最近的[WMDP基准测试](https://www.wmdp.ai/)，对模型进行有害知识的测试，是评估遗忘效果的很好的例子。
- en: 2\. Forms of unlearning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2。遗忘形式
- en: Unlearning is trivially satisfied if we can just retrain the model without the
    undesired data. However, we want something better because (1) retraining can be
    expensive and (2) it can be a lot of work *just to find out* what to remove from
    training data—think finding all Harry Potter references in a trillion tokens.
    Unlearning techniques essentially seek to mitigate or avoid this retraining cost
    while producing identical or similar results.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够仅仅重新训练模型而不使用不需要的数据，那么遗忘就会很轻松满足。然而，我们希望得到更好的结果，因为（1）重新训练可能很昂贵，（2）仅仅为了弄清楚从训练数据中删除什么可能会很费力—想想在一万亿标记中找出所有《哈利·波特》的引用。遗忘技术基本上旨在减轻或避免这种重新训练成本，同时产生相同或相似的结果。
- en: 'The unlearning literature can roughly be categorized into the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘文献可以大致分为以下几类：
- en: Exact unlearning
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精确遗忘
- en: “Unlearning” via differential privacy
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过差分隐私“遗忘”
- en: Empirical unlearning, where data to be unlearned are precisely known (training
    examples)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实证遗忘，其中需要遗忘的数据是准确已知的（训练示例）
- en: Empirical unlearning, where data to be unlearned are underspecified (think “knowledge”)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实证遗忘，其中需要遗忘的数据是不确定的（思考“知识”）
- en: Just *ask* for unlearning?
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*仅仅*要求遗忘吗？'
- en: Forms 2-4 are sometimes known as “**approximate unlearning**” in that the unlearned
    model approximates the behavior of the retrained model. Form 5 is quite new and
    interesting, and more specific to instruction-following models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有时将 2-4 形式称为“**近似遗忘**”，因为遗忘的模型近似于重新训练的模型的行为。第 5 形式是相当新颖和有趣的，更具体地适用于遵循指令的模型。
- en: '![](img/44995bd7d4867fdb4daa34bdbdc2ee86.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44995bd7d4867fdb4daa34bdbdc2ee86.png)'
- en: 'Figure 1\. Illustration of approximate unlearning. Source: [NeurIPS Machine
    Unlearning Challenge](https://unlearning-challenge.github.io).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1。近似遗忘的示意图。来源：[NeurIPS 机器遗忘挑战](https://unlearning-challenge.github.io)。
- en: In the following, we will go through what each of these types roughly looks
    like, along with what I think are the promises, caveats, and questions to ask
    looking forward.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将简要介绍每种类型的大致情况，以及我认为的承诺、注意事项和展望中需要问的问题。
- en: 2.1\. Exact unlearning
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1。精确遗忘
- en: Exact unlearning roughly asks that the unlearned model and the retrained model
    to be *distributionally identical*; that is, they can be exactly the same under
    fixed randomness.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 精确遗忘大致要求遗忘模型和重新训练的模型*在分布上相同*；也就是说，它们可以在固定的随机性下完全相同。
- en: Techniques for exact unlearning are characterized by the early work of [Cao
    & Yang](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)
    and [SISA](https://arxiv.org/abs/1912.03817). In SISA, a very simple scheme, the
    training set is split into $N$ non-overlapping subsets, and a separate model is
    trained for each subset. Unlearning involves retraining the model corresponding
    to and without the data points to be unlearned. This reduces cost from vanilla
    retraining by $1/N$ (cheaper if we keep model checkpoints). Inference then involves
    model ensembling.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 精确取消学习的技术的特点是由[Cao & Yang](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)和[SISA](https://arxiv.org/abs/1912.03817)的早期工作所决定的。在SISA中，采用了一个非常简单的方案，将训练集分成$N$个不重叠的子集，并为每个子集训练一个单独的模型。取消学习涉及到重新训练对应的模型，并且不包含要取消学习的数据点。这将成本从普通的重新训练中降低了$1/N$（如果我们保留模型检查点，则更便宜）。推理然后涉及模型集成。
- en: '![](img/76dbc59f12a3682eb46749c5ee0ed199.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76dbc59f12a3682eb46749c5ee0ed199.png)'
- en: 'Figure 2\. Illustration of SISA: just train models on data shards ([image source](https://arxiv.org/pdf/1912.03817)).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. SISA的说明：只需在数据碎片上训练模型([图片来源](https://arxiv.org/pdf/1912.03817))。
- en: More generally, the essence of exact unlearning of this form is that we want
    *modular components* in the learning algorithm to correspond to different (potentially
    disjoint) sets of the training examples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，这种形式的精确取消学习的本质是，我们希望学习算法中的*模块化组件*对应于训练示例的不同（潜在的不相交）集合。
- en: 'There are several benefits of exact unlearning:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 精确取消学习有几个好处：
- en: '**The algorithm *is* the proof**. If we implement something like SISA, we know
    by design that the unlearned data never contributed to other components. As it
    turns out, formally proving the model has unlearned something is quite [challenging](https://arxiv.org/abs/2110.11891)
    otherwise.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**算法 *即是* 证明**。如果我们实现类似SISA的东西，我们知道按设计，被取消学习的数据从未对其他组件有所贡献。事实证明，正式地证明模型已经取消学习了某些内容在其他情况下是相当具有挑战性的([挑战](https://arxiv.org/abs/2110.11891))。'
- en: '**It turns the unlearning problem into an accuracy/efficiency problem.** This
    makes exact unlearning more approachable due to the messiness of unlearning evaluation
    and lack of benchmarks.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**它将取消学习问题转化为准确性/效率问题。** 由于取消学习评估的混乱和缺乏基准，这使得精确取消学习更容易接近。'
- en: '**Interpretability by design**. By providing a structure to learning, we also
    have better understanding of how certain data points contribute to performance.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设计解释性**。通过为学习提供结构，我们也更好地了解某些数据点如何影响性能。'
- en: 'The main drawback seems obvious: modern [scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law)
    of large models argues against excessive data & model sharding as done in SISA.
    *Or does it?* I think it would be very interesting to revisit sharding in the
    context of large models, in light of the recent [model](https://arxiv.org/abs/2208.03306)
    [merging](https://arxiv.org/abs/2203.05482) [literature](https://huggingface.co/blog/mlabonne/merge-models)
    that suggests the feasibility of weight-space merging between large models. As
    we’ll learn in the coming sections, the messiness of approximate unlearning and
    its evaluation, especially in the context of large models, makes exact unlearning
    very appealing.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 主要缺点似乎很明显：大型模型的现代[扩展定律](https://en.wikipedia.org/wiki/Neural_scaling_law)反对SISA中所做的过度数据和模型分片。*难道是吗？*
    我认为重新审视大型模型的情况下分片是非常有趣的，尤其是考虑到最近的[模型](https://arxiv.org/abs/2208.03306) [合并](https://arxiv.org/abs/2203.05482)
    [文献](https://huggingface.co/blog/mlabonne/merge-models)表明了大型模型之间的权重空间合并的可行性。正如我们将在接下来的部分中了解到的那样，近似取消学习及其评估的混乱，特别是在大型模型的情况下，使得精确取消学习非常有吸引力。
- en: 2.2\. “Unlearning” via differential privacy
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 通过差分隐私进行“取消学习”
- en: 'This line of work roughly says: if the model behaves more or less the same
    with or without any particular data point, then there’s nothing we need to unlearn
    from that data point. More broadly, we are asking for *distributional closeness*
    between the unlearned and the retrained models.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列工作大致上说：如果模型在有或没有任何特定数据点的情况下表现基本相同，那么我们就不需要从该数据点取消学习任何内容。更广泛地说，我们要求未学习和重新训练的模型之间的*分布接近性*。
- en: For readers unfamilar with differential privacy (DP) in machine learning, DP
    defines a *quantifiable* indistinguishability guarantee between two models $M$,
    $M’$ trained on datasets $X$, $X’$ that differ in any single training example.
    The canonical procedure, [DP-SGD](https://arxiv.org/abs/1607.00133), works by
    clipping the L2-norm of the per-example gradients and injecting some per-coordinate
    Gaussian noise to the gradients. The idea is that the noise would mask or obscure
    the contribution of any single gradient (example), such that the final model isn’t
    sensitive any exmaple. It is usually denoted by ($\varepsilon, \delta$)-DP; the
    stronger the noise, the smaller the scalars ($\varepsilon, \delta$), the more
    private.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不熟悉差分隐私（DP）在机器学习中的读者，DP定义了在数据集$X$，$X'$上训练的两个模型$M$，$M'$之间的*可量化*的不可区分性保证，这些数据集在任何单个训练示例上有所不同。经典的过程，[DP-SGD](https://arxiv.org/abs/1607.00133)，通过裁剪每个示例梯度的L2范数并向梯度注入一些每个坐标的高斯噪声来工作。其思想是噪声会掩盖或模糊任何单个梯度（示例）的贡献，使最终模型不会对任何示例敏感。它通常由（$\varepsilon,
    \delta$）-DP表示；噪声越强，标量（$\varepsilon, \delta$）越小，隐私性越强。
- en: The intuition is that if an adversary cannot (reliably) tell apart the models,
    then it is as if this data point has never been learned—thus no need to unlearn.
    DP can be used to achieve this form of unlearning, but due to the one-sidedness
    of unlearning (where we only care about data removal, not addition), DP is a [strictly
    stronger definition](https://arxiv.org/abs/2103.03279). This notion of unlearning
    is sometimes known as “**($\alpha, \beta$)-unlearning**” where ($\alpha, \beta$)
    serve similar roles as ($\varepsilon, \delta$) to measure distributional closeness.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉是，如果对手不能（可靠地）区分模型，那么这个数据点就好像从未被学习过一样，因此不需要取消学习。DP可以用来实现这种形式的取消学习，但由于取消学习的单边性（我们只关心数据的删除，而不关心添加），DP是一个[严格更强的定义](https://arxiv.org/abs/2103.03279)。这种取消学习的概念有时被称为“**（$\alpha,
    \beta$）-取消学习**”，其中（$\alpha, \beta$）类似于用于测量分布接近度的（$\varepsilon, \delta$）。
- en: 'Example techniques along this direction include: [(1)](http://proceedings.mlr.press/v132/neel21a.html)
    storing checkpoints of (DP) convex models and unlearning is retraining from those
    checkpoints; and [(2)](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html)
    on top of the previous technique, add SISA for *adaptive* unlearning requests
    (i.e. those that come in after observing the published model).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着这个方向的示例技术包括：[(1)](http://proceedings.mlr.press/v132/neel21a.html)存储（DP）凸模型的检查点，并且取消学习是从这些检查点重新训练；以及[(2)](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html)在前一项技术的基础上，为*自适应*取消学习请求添加SISA（即在观察到发布的模型后出现的请求）。
- en: 'DP-based unlearning is good in that it gives some form of a statistical guarantee.
    **However, there are some important considerations that limit its applicability
    to large models**:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DP的取消学习之所以好是因为它提供了某种形式的统计保证。**但是，有一些重要考虑因素限制了它对大型模型的适用性**：
- en: Many such unlearning results apply only to [convex](http://proceedings.mlr.press/v132/neel21a.html)
    [models](https://arxiv.org/abs/1911.03030) or [losses](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html).
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多此类取消学习结果仅适用于[凸](http://proceedings.mlr.press/v132/neel21a.html) [模型](https://arxiv.org/abs/1911.03030)或[损失](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html)。
- en: What levels of unlearning (values of $(\varepsilon, \delta)$-DP or $(\alpha,
    \beta)$-unlearning) are sufficient? *Who decides?*
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些取消学习水平（$(\varepsilon, \delta)$-DP或$(\alpha, \beta)$-取消学习的值）是足够的？*谁来决定？*
- en: For large models, current ML systems don’t fit well with the *per-example* workloads
    of DP-like procedures. The memory overhead will also be prohibitive.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大型模型，当前的机器学习系统与DP类似的程序的*每个示例*工作负载不匹配。内存开销也将是禁止的。
- en: Moreoever, like DP, the guarantees can fall off quickly with more unlearning
    requests (at best the [rate](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html)
    of $O(\sqrt k)$ with $k$ requests following DP composition theorems).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，像差分隐私（DP）一样，随着更多的取消学习请求（最佳情况下是$O(\sqrt k)$，其中$k$是遵循DP组合定理的请求），保证可能会迅速减少。
- en: DP-like definitions implicitly assume we care about all data points *equally*.
    But some examples are more likely to receive unlearning request, and some examples
    would not have [contributed](https://arxiv.org/abs/1906.01827) to the learning
    at all.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似DP的定义隐含地假设我们对所有数据点*都一视同仁*。但是，有些例子更有可能收到取消学习请求，而有些例子根本不会[对学习有贡献](https://arxiv.org/abs/1906.01827)。
- en: DP-like procedures may also just hurt model accuracy a lot, sometimes in an
    [unfair](https://arxiv.org/abs/1905.12101) way.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似DP的程序有时也会严重损害模型的准确性，有时以一种[不公平](https://arxiv.org/abs/1905.12101)的方式。
- en: For large models in particular, it’s also worth distinguishing the cases of
    **unlearning pre-training data** vs **unlearning fine-tuning data**. The latter
    is a lot more tractable; for example, we could indeed [fine-tune](https://arxiv.org/abs/2110.05679)
    large models with differential privacy but not so much with pre-training.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于大型模型，值得区分**取消学习预训练数据**与**取消学习微调数据**的情况。后者更容易处理；例如，我们确实可以使用微分隐私对大型模型进行微调，但对于预训练来说则不太容易。
- en: 2.2.1\. *Forging* and its implications on DP-like unlearning definitions
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1\. *伪造*及其对类似DP取消学习定义的影响
- en: An unlearning procedure may sometimes require an external *audit*, meaning that
    we’d like to prove that the unlearning procedure has actually happened.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 取消学习过程有时可能需要外部*审计*，意味着我们希望证明取消学习过程确实已发生。
- en: 'The main idea of “[forging](https://arxiv.org/abs/2110.11891)” is that there
    exists two distinct datasets that, when trained on, would produce *the same gradients
    and (thus) the same models*. This is true intuitively:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: “[伪造](https://arxiv.org/abs/2110.11891)”的主要思想是存在两个不同的数据集，当对其进行训练时，将产生*相同的梯度和（因此）相同的模型*。这在直觉上是正确的：
- en: Think linear regression of points on a perfect line; removing any 1 point doesn’t
    change the fitted line;
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将点线性回归至完美直线；移除任意一个点都不会改变拟合的直线；
- en: Think mini-batch GD, where replacing one example gradient with the sum of several
    “fake” gradients would give the same batch gradient.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想象一下小批量GD，其中将一个示例梯度替换为几个“假”梯度的总和会产生相同的批量梯度。
- en: '**Forging implies that DP-based approximate unlearning may not be auditable**—that
    is, the unlearning service provider cannot formally prove that the forget set
    is really forgotten. In fact, if we only look at the model weights, even exact
    unlearning may not be auditable.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**伪造意味着基于DP的近似取消学习可能无法通过审核**——也就是说，取消学习服务提供者无法正式证明遗忘集确实被遗忘了。事实上，如果我们只看模型权重，即使是精确取消学习也可能无法通过审核。'
- en: While one can brush this off as a theoretical result, it does mean that policymakers
    should think carefully about how a future version of “right-to-be-forgotten” (if
    any) should look like and whether similar policies are legally and technically
    enforceable.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有人可能认为这是一个理论结果，但这意味着政策制定者应该仔细考虑未来“被遗忘权”（如果有的话）应该是什么样子，以及类似的政策是否在法律上和技术上可执行。
- en: Indeed, what qualifies as an “audit” could very well be definition and application
    dependent. If the auditor only cares that the unlearned model performs poorly
    on a specified set of inputs (say on a set of face images), then even empirical
    unlearning is “auditable” (see next section).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，什么样的“审计”合格可能会很大程度上取决于定义和应用。如果审计员只关心取消学习模型在指定输入集上的表现（例如在一组面部图像上），那么即使是经验性取消学习也是“可审计的”（见下一节）。
- en: 2.3\. Empirical unlearning with known example space (“example unlearning”)
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 已知示例空间的实证取消学习（“示例取消学习”）
- en: 'This line of work is essentially “training to unlearn” or “unlearning via fine-tuning”:
    just take a few more heuristically chosen gradient steps to shape the original
    model’s behavior into *what we think* the retrained model would do (while also
    optionally resetting some parameters in the model). It may also be referred to
    as “example unlearning”, since the training, retain, and forget sets are often
    clearly defined.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这一工作线路本质上是“训练以取消学习”或“通过微调取消学习”：只需采取几个启发式选择的梯度步骤来塑造原始模型的行为为*我们认为*重新训练的模型会做的事情（同时也可以选择重置模型中的一些参数）。它也可以被称为“示例取消学习”，因为训练、保留和遗忘集通常是明确定义的。
- en: 'The [**NeurIPS 2023 Machine Unlearning Challenge**](https://unlearning-challenge.github.io/)
    collected many methods along this direction. The challenge roughly runs as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[**NeurIPS 2023 机器取消学习挑战**](https://unlearning-challenge.github.io/)收集了许多沿着这个方向的方法。挑战大致如下进行：'
- en: You are given a face image dataset with designated retain/forget example splits
    for the training set, a target model trained on everything, and a secret model
    trained only on the retain set.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将获得一个带有指定保留/遗忘示例拆分的面部图像数据集，一个在所有内容上进行训练的目标模型，以及一个仅在保留集上训练的秘密模型。
- en: You are asked to design an unlearning algorithm that produces unlearned model(s)
    from the target model that “match” the secretly kept model.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求您设计一个取消学习算法，从目标模型生成与秘密保留模型“匹配”的取消学习模型。
- en: 'The “match” or evaluation metric uses a DP-like output-space similarity over
    512 seeds: for each forget example, compute an “empirical $\varepsilon$” over
    512 unlearned models based on true/false positive rates of an adversary (also
    provided by the organizer), and aggregate across examples.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “匹配”或评估指标在512个种子上使用类似DP的输出空间相似性：对于每个遗忘示例，基于对手的真/假正例率在512个未学习模型上计算一个“经验性 $\varepsilon$”，并在示例之间进行聚合。
- en: All models are a small ConvNet.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型都是小型卷积网络。
- en: 'To give an intuition about how well empirical unlearning is doing without fully
    explaining the metric: the ground-truth retrained model gets about ~0.19, the
    winning submission gets to ~0.12, and the baseline (simple gradient ascent on
    forget set) is ~0.06.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地说明经验性遗忘的效果，而不完全解释指标：地面实况重新训练模型大约得到~0.19，获胜的提交达到了~0.12，而基线（在遗忘集上进行简单的梯度上升）为~0.06。
- en: 'So what do the winning ideas look like? Something along the lines of the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 那么获胜的想法是什么样的呢？大致如下：
- en: Gradient ascent on the forget set;
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在遗忘集上进行梯度上升；
- en: Gradient descent on the retain set (and hope that catastrophic forgetting takes
    care of unlearning);
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保留集上进行梯度下降（并希望灾难性遗忘会处理未学习的内容）；
- en: Gradient descent on the forget set, but with uniformly random labels (to “confuse”
    the model);
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在遗忘集上进行梯度上升，但标签是均匀随机的（以“混淆”模型）；
- en: Minimize KL divergence on outputs between unlearned model and original model
    on the retain set (to regularize unlearned model performance on unrelated data);
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保留集上最小化未学习模型和原始模型输出之间的KL散度（以规范化未学习模型在不相关数据上的性能）；
- en: Re-initialize weights that had similar gradients on the retain set and forget
    sets, and finetune these weights on the retain set;
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新初始化在保留集和遗忘集上具有相似梯度的权重，并在保留集上微调这些权重；
- en: Prune 99% of weights by L1-norm and fine-tune on the retain set;
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过L1范数将99%的权重修剪掉，并在保留集上进行微调；
- en: Reset first/last $k$ layers and fine-tune on the retain set; and
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置第一个/最后一个 $k$ 层并在保留集上进行微调；以及
- en: Heuristic/arbitrary combinations of the above.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述启发式/任意组合。
- en: Indeed, despite the heuristic nature of these approaches, these are what [most](https://arxiv.org/abs/2302.09880)
    [empirical](https://arxiv.org/abs/2210.01504) [unlearning](https://arxiv.org/abs/2305.06535)
    [algorithms](https://arxiv.org/abs/2201.06640), especially those [on](https://arxiv.org/abs/2310.10683)
    [large](https://arxiv.org/abs/2310.20150) [(language)](https://www.microsoft.com/en-us/research/project/physics-of-agi/articles/whos-harry-potter-making-llms-forget-2/)
    [models](https://arxiv.org/abs/2403.03218), are doing these days.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，尽管这些方法的启发式性质，但这些是目前[大多数](https://arxiv.org/abs/2302.09880) [经验性](https://arxiv.org/abs/2210.01504)
    [遗忘](https://arxiv.org/abs/2305.06535) [算法](https://arxiv.org/abs/2201.06640)，特别是那些[在](https://arxiv.org/abs/2310.10683)
    [大型](https://arxiv.org/abs/2310.20150) [（语言）](https://www.microsoft.com/en-us/research/project/physics-of-agi/articles/whos-harry-potter-making-llms-forget-2/)
    [模型](https://arxiv.org/abs/2403.03218)，如今正在做的事情。
- en: People explore empirical approaches because theoretical tools are usually impractical;
    for example, enforcing DP simply hurts accuracy and efficiency too much, even
    for the GPU rich. On the flip side, empirical methods are often fast and easy
    to implement, and their effects are often qualitatively visible.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 人们探索经验性方法是因为理论工具通常不实用；例如，简单地强制执行DP会大大降低准确性和效率，即使对于拥有丰富GPU的情况也是如此。另一方面，经验性方法通常快速且易于实现，并且它们的效果通常在定性上是可见的。
- en: Another key motivation for empirical unlearning is that *counterfactuals* are
    unclear, especially on LLMs. In deep learning, we often don’t know how the retrained
    model would behave on unseen data. What should the LLM think who Biden is, if
    not a politician? Should image classifiers give uniformly random predictions for
    unlearned images? Do they generalize? Or are they confidently wrong? Any of these
    is possible and it can be up to the practitioner to decide. It also means that
    *behaviors that are equally plausible can lead to wildly different measurements*
    (e.g., KL divergence between output distributions of unlearned & retrained model),
    complicating theoretical guarantees.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 经验性遗忘的另一个关键动机是*反事实*不清楚，特别是在LLMs上。在深度学习中，我们经常不知道重新训练的模型在未见数据上的行为。如果不是政治家，LLM应该认为拜登是什么？图像分类器对未学习的图像是否应该给出均匀随机的预测？它们是否泛化？或者它们是否自信地错误？这些都是可能的，而且可以由从业者决定。这也意味着*同样合理的行为可能导致截然不同的测量*（例如，未学习和重新训练模型输出分布之间的KL散度），从而使理论保证变得更加复杂。
- en: 2.4\. Empirical unlearning with unknown example space (“concept/knowledge unlearning”)
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 对未知示例空间进行经验性遗忘（“概念/知识遗忘”）
- en: What if the train, retain, or forget sets are poorly specified or just not specified
    at all? Foundation models that train on internet-scale data may get requests to
    unlearn a “[concept](https://openaccess.thecvf.com/content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf)”,
    a “[fact](https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf)”,
    or a piece of “[knowledge](https://arxiv.org/abs/2305.14795)”, all of which we
    cannot easily associate a set of examples. The terms “**model editing**”, “**concept
    editing**”, “**model surgery**”, and “**knowledge unlearning**” are closely related
    to this notion of unlearning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列车、保留或遗忘集指定不好或根本不指定会怎么样？训练在互联网规模的数据上的基础模型可能会收到取消学习“[概念](https://openaccess.thecvf.com/content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf)”，“[事实](https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf)”，或一条“[知识](https://arxiv.org/abs/2305.14795)”的要求，这些要求我们无法轻易关联一组示例。术语“**模型编辑**”、“**概念编辑**”、“**模型手术**”和“**知识取消学习**”与取消学习的这种概念密切相关。
- en: The underspecification of the unlearning requests means that we now have to
    deal with the notions of “**unlearning scope**” (or “[editing scope](https://arxiv.org/abs/2206.06520)”)
    and “[**entailment**](https://en.wikipedia.org/wiki/Textual_entailment)”. That
    is, unlearning requests may provide [canonical examples](https://arxiv.org/abs/2402.06155)
    to indicate what to unlearn, but the same information can manifest in the (pre-)training
    set in many different forms with many different downstream implications such that
    simply achieving unlearning on these examples—even *exactly*—would not suffice.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 取消学习请求的不明确意味着我们现在必须处理“**取消学习范围**”（或“[编辑范围](https://arxiv.org/abs/2206.06520)”）和“[**蕴涵**](https://en.wikipedia.org/wiki/Textual_entailment)”的概念。也就是说，取消学习请求可能提供[典型示例](https://arxiv.org/abs/2402.06155)以指示要取消学习的内容，但相同的信息可以以许多不同的形式和许多不同的下游影响出现在（预）训练集中，因此仅仅实现这些示例的取消学习，即使*完全*也是不够的。
- en: 'For example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: The association “Biden is the US president” is dispersed throughout various
    forms of text from news articles, books, casual text messages, or this very blog
    post. Can we ever unlearn all occurrences? Moreover, does unlearning Joe Biden
    also entail unlearning the color of [Biden’s cat](https://en.wikipedia.org/wiki/Willow_(cat))?
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于“拜登是美国总统”的关联信息分散在来自新闻文章、书籍、日常短信或本文的各种形式的文本中。我们是否可以取消学习所有出现的情况？此外，取消学习乔·拜登是否也意味着取消学习[拜登的猫](https://en.wikipedia.org/wiki/Willow_(cat))的颜色？
- en: Artists may request to unlearn art style by providing art samples, but they
    won’t be able to collect everything they have on the internet and their [adaptations](https://www.businessinsider.com/ai-image-generators-artists-copying-style-thousands-images-2022-10).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艺术家可能要求通过提供艺术样本来取消学习艺术风格，但他们可能无法收集到他们在互联网上拥有的一切和他们的[改编作品](https://www.businessinsider.com/ai-image-generators-artists-copying-style-thousands-images-2022-10)。
- en: New York Times may request to unlearn news articles, but they cannot enumerate
    quotes and secondary transformations of these articles.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纽约时报可能要求取消学习新闻文章，但他们不能列举这些文章的引用和二次转换。
- en: Such vagueness also suggests that **unlearning pre-training data from large
    models are perhaps necessarily empirical:** it is unlikely to derive formal guarantees
    if we can’t clearly specify what to (and what not to) unlearn in the trillions
    of tokens and establish clear information boundaries between different entities.
    An interesting implication of achieving unlearning empirically is that the unlearning
    *itself* can be [unlearned](https://arxiv.org/abs/2401.01814).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种含糊也表明，**从大型模型中取消预训练数据可能是必须经验性的：**如果我们无法清楚地指定要（和不要）在数万亿标记中取消学习，并在不同实体之间建立明确的信息边界，那么很难推导出正式的保证。通过经验性地实现取消学习的一个有趣的含义是，*取消学习本身*可以被[取消学习](https://arxiv.org/abs/2401.01814)。
- en: What does existing work do, then, with underspecified unlearning requests? Most
    techniques are more or less the same as [before](#33-Empirical-unlearning-or-unlearning-via-fine-tuning),
    except now we also need to find the examples to fine-tune on. For example, attempting
    to unlearn [Harry Potter](https://arxiv.org/abs/2310.02238) involves asking GPT-4
    to come up with plausible alternative text completions (e.g. that Mr. Potter studies
    baking instead of magic); and attempting to unlearn [harmful behavior](https://arxiv.org/abs/2310.10683)
    involves collecting examples of hatespeech.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现有工作如何处理未明确说明的遗忘请求呢？大多数技术或多或少与[之前](#33-Empirical-unlearning-or-unlearning-via-fine-tuning)相同，只是现在我们还需要找到要进行微调的示例。例如，试图遗忘[哈利·波特](https://arxiv.org/abs/2310.02238)涉及要求GPT-4提出可信的替代文本完成（例如，波特先生改为研究烘焙而不是魔法）；而试图遗忘[有害行为](https://arxiv.org/abs/2310.10683)则需要收集仇恨言论的例子。
- en: Another set of techniques involves training the desired behavior (or its opposite)
    into [task](https://arxiv.org/abs/2212.04089)/[control](https://arxiv.org/abs/2403.03218)
    [vectors](https://arxiv.org/abs/2306.14870) and harnessing the capability of large
    models to undergo [weight-space merging](https://huggingface.co/docs/peft/en/developer_guides/model_merging)
    or [activation steering](https://arxiv.org/abs/2310.01405). The fundamental approach
    of the above is more or less the same, nevertheless—obtaining these edit vectors
    involves (heuristically) designing what gradients to take and what data on which
    to take them. One could also frame the unlearning problem as an *alignment* problem
    and applies the forget examples with a [DPO-like objective](https://arxiv.org/abs/2404.05868).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组技术涉及将所需的行为（或其相反）训练到[任务](https://arxiv.org/abs/2212.04089)/[控制](https://arxiv.org/abs/2403.03218)[向量](https://arxiv.org/abs/2306.14870)中，并利用大型模型具备的能力进行[权重空间合并](https://huggingface.co/docs/peft/en/developer_guides/model_merging)或[激活转向](https://arxiv.org/abs/2310.01405)。以上的基本方法或多或少是相同的，然而——获取这些编辑向量涉及（启发式地）设计要采取的梯度和采取这些梯度的数据。人们还可以将遗忘问题框定为一个*对齐*问题，并应用[DPO类似的目标](https://arxiv.org/abs/2404.05868)来忘记例子。
- en: 2.5\. Just *ask* for unlearning?
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 只是*要求*遗忘？
- en: It turns out that powerful, instruction-following LLMs like GPT-4 are smart
    enough to *pretend to unlearn*. This means crafting prompts to induce a (sufficiently)
    safe behavior for the target unlearning application.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，像GPT-4这样强大的、遵循指令的LLMs足够聪明，可以*假装遗忘*。这意味着制定提示来诱导（足够）安全的行为，以用于目标遗忘应用程序。
- en: This is an interesting approach because no gradients are involved whatsoever
    (big plus from a systems perspective), and intuitively the end results could very
    well be [as good as](https://arxiv.org/abs/2403.03329) existing empirical unlearning
    techniques. Among different ways we could prompt, past work explored the following
    two directions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种有趣的方法，因为从系统的角度来看，完全没有涉及到任何梯度（这是一个很大的优势），直观上最终的结果可能会像现有的经验性遗忘技术一样[好](https://arxiv.org/abs/2403.03329)。在我们可以提示的不同方式中，过去的工作探索了以下两个方向。
- en: '**[Literally asking](https://arxiv.org/abs/2403.03329) to pretend unlearning.**
    We can ask in the system prompt to, say, pretend to not know who Harry Potter
    is. By design, this works best for common entities, facts, knowledge, or behaviors
    (e.g. the ability to utter like Trump) that are well-captured in the pre-training
    set, since the LLM needs to *know it well to pretend not knowing it well*. On
    the other hand, suppose now we’d like to unlearn the address of an obscure person;
    the pre-training set is so large that we suspect it’s part of training data. We
    now face a variant of the [Streisand effect](https://en.wikipedia.org/wiki/Streisand_effect):
    is it even worth asking the model to pretend unlearning by accurately describing
    it in-context, and subsequently [risk leaking it](https://arxiv.org/abs/2402.17840)
    in subsequent model responses?'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**[直接询问](https://arxiv.org/abs/2403.03329)**来假装遗忘。我们可以在系统提示中询问，比如，假装不知道谁是哈利·波特。从设计上来说，这对于常见的实体、事实、知识或行为（例如像特朗普那样说话的能力）效果最佳，因为LLM需要*很好地知道它才能假装不知道它很好*。另一方面，假设现在我们想要遗忘一个不知名人士的地址；预训练集如此之大，以至于我们怀疑它是训练数据的一部分。我们现在面临的是[斯特莱桑德效应](https://en.wikipedia.org/wiki/Streisand_effect)的一个变体：甚至值得问模型通过准确地描述它的上下文来假装遗忘，并随后[冒泄露的风险](https://arxiv.org/abs/2402.17840)吗？'
- en: '**Few-shot prompting or “[in-context unlearning](https://arxiv.org/abs/2310.07579)”.**
    Suppose we now have a clearly defined set of forget examples with corresponding
    labels. We can *flip* their labels and put them in the prompt, along with more
    retain examples with correct labels, with the intuition that the model would treat
    these falsely labelled forget examples as truths and act accordingly—much like
    one could [jailbreak](https://www.anthropic.com/research/many-shot-jailbreaking)
    a model this way. ^(Indeed, this works best when the forget examples and the counterfactual
    labels are clearly defined and (somewhat) finite. It *may* work for factual associations
    (e.g. Paris is the captial of France) by enumerating a *lot* of examples, but
    unlikely to work for unlearning toxic behaviors (where space of possible outputs
    is much larger).)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**少量提示或“[上下文取消学习](https://arxiv.org/abs/2310.07579)”**。假设现在我们有一组明确定义的忘记示例及其相应的标签。我们可以*翻转*它们的标签，并将它们与更多具有正确标签的保留示例一起放入提示中，直觉是模型会将这些错误标记的忘记示例视为真实并据此行事——就像一个人可以用这种方式[越狱](https://www.anthropic.com/research/many-shot-jailbreaking)一个模型一样。
    ^(实际上，当忘记示例和反事实标签明确定义且（在某种程度上）有限时，这种方法效果最好。通过枚举*大量*示例，它*可能*适用于事实关联（例如，巴黎是法国的首都），但不太可能适用于取消学习有毒行为（其中可能输出的空间要大得多）)。'
- en: In a sense, these approaches are complementary as they work for different kinds
    of unlearning requests.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，这些方法是互补的，因为它们适用于不同类型的取消学习请求。
- en: '**More broadly, one could imagine a *boxed* LLM system for unlearning through
    prompting, where**:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**更广泛地说，人们可以想象一个用于取消学习的*盒装*LLM系统，其中**：'
- en: Only the input and output interfaces are exposed (like ChatGPT);
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有输入和输出接口是公开的（就像ChatGPT）；
- en: Different instances of a powerful LLM are responsible for accurately mimicking
    different parts of a desired unlearning behavior (for example, one LLM instance
    specializes in general trivia-style QA while anoother handles sequence completions);
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强大的LLM的不同实例负责准确模拟所需取消学习行为的不同部分（例如，一个LLM实例专门处理一般的琐事式QA，而另一个处理序列完成）；
- en: An orchestrator/router LLM decides which unlearning worker instance to call
    depending on the input; and
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个编排者/路由器LLM根据输入决定调用哪个取消学习工作者实例；和
- en: A composer/summarizer LLM that drafts the final output conforming to the desired
    unlearning behavior; it may also apply some output filtering.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个作曲家/总结者LLM，起草符合所需取消学习行为的最终输出；它也可以应用一些输出过滤。
- en: 'Some readers may grumble about the heuristic nature of such prompting-based
    techniques; that there is no proof of unlearning whatsoever. We should keep in
    mind that fine-tuning based empirical unlearning, as most recent approaches do,
    is perhaps not fundamentally different. I think it ultimately comes down to the
    following questions:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有些读者可能会对这种基于启发式的提示技术的本质抱怨；即没有任何取消学习的证据。我们应该记住，像最近的方法一样基于经验的微调取消学习可能并不根本不同。我认为最终归结为以下问题：
- en: Which of fine-tuning or prompting can **better steer model behavior**?
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调或提示哪个能**更好地引导模型行为**？
- en: Which of them are **less susceptible to attacks** (exposing less surfaces and/or
    requiring more effort for an adversary to revert the unlearning)?
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其中哪些**更不容易受到攻击**（暴露的表面更少和/或需要对手更多的努力来恢复取消学习）？
- en: My intuition of our current models says that both questions point to fine-tuning
    based unlearning, but this is very much up for debate and can change as we get
    more powerful models and better defense mechanisms. For example, the recent notion
    of an [instruction hierarchy](https://arxiv.org/abs/2404.13208) may help make
    such as an LLM system less susceptible to malicious prompts.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我对我们当前模型的直觉是，这两个问题都指向基于微调的取消学习，但这在辩论中非常有待商榷，并且随着我们获得更强大的模型和更好的防御机制而可能发生变化。例如，最近提出的[指令层次结构](https://arxiv.org/abs/2404.13208)的概念可能有助于使这样的LLM系统不那么容易受到恶意提示的影响。
- en: '**It might be useful to note that humans don’t really “unlearn” a piece of
    knowledge either.** In fact, by claiming to have unlearned something, we often
    have: (1) not only learned it well to be able to make the very claim that we have
    unlearned it, and (2) consciously decided that it’s no longer useful / beneficial
    to apply this knowledge to our current world state. Who is to say that unlearning
    for LLMs should be any different?'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**值得注意的是，人类实际上也不会“取消学习”一段知识。**实际上，通过声称已经取消学习某事，我们通常：（1）不仅仅是学会了它以便能够做出我们已经取消学习它的声明，并且（2）有意识地决定不再将这些知识应用于我们当前的世界状态。谁又能说LLM的取消学习应该有什么不同呢？'
- en: 3\. Evaluating unlearning
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 评估取消学习
- en: 'Unlearning is messy for many reasons. But one of the biggest broken things
    about unlearning is evaluation. In general, we care about three aspects:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多原因，遗忘是混乱的。但关于遗忘的最大问题之一是评估。一般来说，我们关心三个方面：
- en: '**Efficiency**: how fast is the algorithm compared to re-training?'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：与重新训练相比，算法有多快？'
- en: '**Model utility**: do we harm performance on the retain data or orthogonal
    tasks?'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型效用**：我们是否损害了对保留数据或正交任务的性能？'
- en: '**Forgetting quality**: how much of the “forget data” is actually unlearned?
    How fast can we recover (re-learn) them?'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘质量**：多少“遗忘数据”实际上是未被学习的？我们能多快地恢复（重新学习）它们？'
- en: Evaluating efficiency and model utility are easier; we already measure them
    during training. The key challenge is in understanding the forgetting quality.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 评估效率和模型效用更容易；我们在训练期间已经对它们进行了测量。关键挑战在于理解遗忘质量。
- en: If the forget examples are specified, this feels easy too. For example, unlearning
    a particular image class intuitively means getting a near-chance accuracy on the
    images in that class. An evaluation protocol may measure [accuracy](https://arxiv.org/abs/2310.20150)
    (high on retain & test set, low on forget set) or the [likelihood](https://arxiv.org/abs/2210.01504)
    of the forget text sequences (lower the better).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了遗忘示例，这也感觉很容易。例如，直觉上，遗忘特定图像类别意味着在该类别的图像上获得接近机会的准确率。评估协议可以测量[准确性](https://arxiv.org/abs/2310.20150)（在保留和测试集上高，在遗忘集上低）或遗忘文本序列的[可能性](https://arxiv.org/abs/2210.01504)（越低越好）。
- en: However, these intuitive choices of metrics aren’t necessarily principled or
    extensible to settings like knowledge unlearning in LLMs. Expecting the model
    to perform poorly on an unlearned image ignores *generalization*, as the forget
    examples could very well be an interpolation/duplicate of certain retain examples.
    And we don’t always have oracle models that have never seen the forget examples;
    e.g., do we have LLMs that have never seen New York Times articles?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些直观的指标选择未必是原则性的或可扩展到LLMs中的知识遗忘设置。期望模型在遗忘的图像上表现不佳忽视了**泛化**，因为遗忘示例很可能是某些保留示例的插值/重复。我们并不总是有从未见过遗忘示例的oracle模型；例如，我们是否有从未见过《纽约时报》文章的LLMs？
- en: Evaluating unlearning on LLMs had been more of an art than science. For example,
    to unlearn “Harry Potter” as an entity, people would [visualize](https://www.microsoft.com/en-us/research/project/physics-of-agi/articles/whos-harry-potter-making-llms-forget-2/)
    how the token probabilities would decay for Harry Potter related text—and some
    other folks would come along and show that the model can indeed still [answer](https://swj0419.github.io/detect-pretrain.github.io/)
    Harry Potter trivia questions. The key issue has been the *desperate* lack of
    datasets and benchmarks for unlearning evaluation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLMs的遗忘进行评估更像是一门艺术而不是科学。例如，要忘记“哈利·波特”作为一个实体，人们会[可视化](https://www.microsoft.com/en-us/research/project/physics-of-agi/articles/whos-harry-potter-making-llms-forget-2/)标记概率如何因与哈利·波特相关的文本而衰减—然后其他人会过来展示模型确实仍然可以[回答](https://swj0419.github.io/detect-pretrain.github.io/)哈利·波特的小知识问答。关键问题是对于遗忘评估的数据集和基准的**迫切**缺乏。
- en: 'Since 2024, nevertheless, the benchmarking crisis is getting better. There
    are two recent projects worth highlighting:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自2024年以来，基准评估危机正在好转。有两个值得关注的最近项目：
- en: '[TOFU](https://locuslab.github.io/tofu/): A benchmark focusing on unlearning
    individuals (specifically book authors). It involves asking GPT-4 to create fake
    author profiles, fine-tuning an LLM on them, and using the fine-tune as the unlearning
    target model and the original LLM as the oracle “retrained” model. It provides
    QA pairs on the generated fake authors to evaluate a model’s knowledge of these
    authors before/after applying unlearning.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TOFU](https://locuslab.github.io/tofu/)：一个专注于遗忘个体（特别是书籍作者）的基准。它涉及要求GPT-4创建虚假的作者档案，对它们进行微调，使用微调后的LLM作为遗忘目标模型，将原始LLM作为oracle“重新训练”模型。它提供了关于生成的虚假作者的QA对，以评估模型在应用遗忘前/后对这些作者的知识。'
- en: '[WMDP](https://www.wmdp.ai/): A benchmark focusing on unlearning dangerous
    knowledge, specifically on biosecurity, cybersecurity, and chemical security.
    It provides 4000+ multiple-choice questions to test a model’s hazardous knowledge
    before/after applying unlearning. As part of the report the authors also propose
    an activation steering based empirical unlearning method.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WMDP](https://www.wmdp.ai/)：一个关注危险知识遗忘的基准，特别关注生物安全、网络安全和化学安全。它提供了4000多个多项选择题，用于测试模型在应用遗忘前/后的危险知识。作为报告的一部分，作者还提出了一种基于激活导向的经验遗忘方法。'
- en: TOFU and WMDP depart from previous unlearning evaluation in that they are both
    “high-level” and focus on the model’s *knowledge retention and understanding*
    as opposed to example-level metrics like forget sequence perplexity. This is particularly
    relevant for LLMs as they are generally capabale of giving the same answer in
    many different ways that example-level metrics can’t capture.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: TOFU和WMDP与先前的忘却评估不同之处在于它们都是“高级”的，并且侧重于模型的*知识保留和理解*，而不是像忘记序列困惑度这样的例级指标。这对于LLMs尤为重要，因为它们通常能够以许多不同的方式给出相同的答案，而例级指标无法捕捉到这一点。
- en: Looking forward, I think **application-oriented unlearning benchmarks** like
    TOFU and WMDP, as opposed to instance-based evaluation like that of the [NeurIPS
    unlearning challenge](https://unlearning-challenge.github.io/), are more useful
    for evaluating foundation models, owing to the multi-tasking nature of these models
    and the disparate definitions of “unlearning success” for each of these tasks.
    Indeed, one might imagine separate benchmarks on unlearning personally identifiable
    information (PII), copyrighted content, speech toxicity, or even model [backdoors](https://arxiv.org/abs/2401.05566).
    For example, for unlearning PII, we might care about exact token regurgitation,
    whereas for toxicity, the unlearning metric would be the score reported by a [ToxiGen](https://arxiv.org/abs/2203.09509)
    classifier.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，我认为像TOFU和WMDP这样的**面向应用的忘却基准**，与[NeurIPS忘却挑战](https://unlearning-challenge.github.io/)的基于实例的评估不同，对于评估基础模型更加有用，因为这些模型具有多任务性质，而且对于这些任务的“忘却成功”的不同定义。事实上，人们可以想象对PII、受版权保护的内容、语音毒性甚至模型[后门](https://arxiv.org/abs/2401.05566)进行单独的忘却基准。例如，对于忘却PII，我们可能关心的是确切的令牌重复，而对于毒性，忘却指标将是由[ToxiGen](https://arxiv.org/abs/2203.09509)分类器报告的得分。
- en: 4\. Practice, pitfalls, and prospects of unlearning
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 忘却的实践、陷阱和前景
- en: Unlearning is a hard problem, especially in the context of foundation models.
    As we actively research to make unlearning work in practice, it helps to philosophize
    a bit on what unlearning really means and whether it is the right solution for
    our current problems.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 忘却是一个难题，特别是在基础模型的背景下。当我们积极研究如何使忘却在实践中起作用时，对于忘却的真正含义以及它是否是当前问题的正确解决方案进行一些哲学思考是有帮助的。
- en: '**4.1\. The spectrum of unlearning hardness**'
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**4.1\. 忘却难度的光谱**'
- en: Intuitively, unlearning infrequent textual occurrences in LLMs like car accidents
    in Palo Alto should be easier than unlearning frequent occurrences like “Biden
    is the US president”, which is in turn easier than unlearning fundamental facts
    like “the sun rises every day”.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上，像Palo Alto中的车祸这样的LLMs中不频繁的文本出现应该比像“拜登是美国总统”这样的频繁出现更容易，而后者又比“太阳每天升起”这样的基本事实更容易。
- en: This spectrum of *unlearning hardness* emerges because as a piece of knowledge
    becomes more fundamental, it will have more associations with other pieces of
    knowledge (e.g. as premises or corollaries) and an exponentially larger unlearning
    scope. In fact, a piece of knowledge can be so embedded in the model’s implicit
    knowledge graph that it cannot be unlearned without *introducing contraditions*
    and harming the model’s utility.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种*忘却难度的光谱*之所以出现，是因为随着一段知识变得更加基础，它将与其他知识片段（例如前提或推论）有更多的关联，并且存在指数级别的更大的忘却范围。事实上，一段知识可能在模型的隐式知识图中被如此嵌入，以至于无法在不*引入矛盾*并损害模型效用的情况下忘记。
- en: This intuition implies that certain unlearning requests are much harder or simply
    unsatisfiable (any attempts are bound to have flaws). Indeed, humans have experiences
    that form the basis of their subsequent actions and world models; it is subjective,
    blurry, and philosophical as to what capacity can humans unlearn their formative
    past memories.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直觉暗示着某些忘却请求要困难得多，或者根本无法满足（任何尝试都注定会有缺陷）。事实上，人类有经验，这些经验构成了他们后续行动和世界模型的基础；就人类能够忘记他们形成的过去记忆的能力而言，这是主观的、模糊的和哲学性的。
- en: 'More broadly, the unlearning hardness problem applies to all kinds of models,
    and for reasons beyond embeddedness in a knowledge/entailment graph. Let’s consider
    two more seemingly contradictory intuitions for unlearning hardness:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，忘却难度问题适用于所有类型的模型，并且出于嵌入在知识/蕴含图中的原因以外的原因。让我们考虑两种看似矛盾的关于忘却难度的直觉：
- en: An example seen later in the training should be *easy to unlearn*, since the
    model would have moved only slightly in weight space (e.g. due to decayed learning
    rate) and one could either just revert gradients or revert to a previous checkpoint
    (if stored). In contrast, examples seen early gets “built on” by later examples
    (in the curriculum learning sense), making them harder to unlearn.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后见到的示例应该是*容易被遗忘*的，因为模型在权重空间中只会微微移动（例如由于衰减的学习速率），可以只需恢复梯度或回滚到先前的检查点（如果已存储）。
    相比之下，早期见到的示例会被后来的示例“建立在其上”（在课程学习的意义上），使它们更难以被遗忘。
- en: An example seen later should be *harder to unlearn*, since examples seen earlier
    are gradually (or catastrophically) [forgotten](https://arxiv.org/abs/2207.00099)
    over the course of training; this may be especially true for LLMs.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后来见到的示例应该是*更难被遗忘*的，因为早期见到的示例会在训练过程中逐渐（或灾难性地）[被遗忘](https://arxiv.org/abs/2207.00099)；这对LLMs可能尤其正确。
- en: Failure to reconcile these intuition would suggest that the interplay across
    *memorization/forgetting*, *example importance* (in the sense of data [selection](https://arxiv.org/abs/2305.10429)
    and [coresets](https://arxiv.org/abs/1906.01827)), *learning hardness* (in the
    sense of [prediction flips](https://arxiv.org/abs/2210.15031)), and unlearning
    hardness is unclear.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 未能协调这些直觉会表明，对*记忆/遗忘*、*示例重要性*（在数据[选择](https://arxiv.org/abs/2305.10429)和[核心集](https://arxiv.org/abs/1906.01827)的意义上）、*学习难度*（在[预测翻转](https://arxiv.org/abs/2210.15031)的意义上），以及遗忘难度之间的相互作用是不明确的。
- en: '**Here are some interesting research questions**:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下是一些有趣的研究问题**：'
- en: Is there a qualitative/fundamental difference between unlearning “easy” data
    (e.g. a local news event) and “hard” data (e.g. cats have four legs)?
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有遗忘“容易”数据（例如地方新闻事件）和“困难”数据（例如猫有四条腿）之间的质量/基本差异？
- en: If there is a spectrum of unlearning hardness, does there exist a threshold
    to tell apart what is “easy” and “hard”, and thus what is unlearnable or shouldn’t
    be unlearned? Does there exist, or can we train, such an oracle classifier? Can
    humans even tell?
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在一个遗忘难度的光谱，是否存在一个阈值来区分“容易”和“困难”，因此决定何为不可遗忘或不应遗忘？是否存在，或者我们能够训练出这样一个预言分类器？甚至人类能分辨吗？
- en: How does this relate to **influence functions** and **data attribution**? If
    a certain piece of knowledge (as it manifests in a model’s output) can be attributed
    to a larger fraction of the training data, does it make it harder to unlearn?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这如何与**影响函数**和**数据归因**相关？如果某一知识点（如它在模型输出中的体现）可以归因于更大一部分的训练数据，这是否使它更难以被遗忘？
- en: Can we benchmark how easy is it to unlearn something?
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否评估遗忘某事的难度？
- en: 4.2\. Copyright protection
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 版权保护
- en: 'On the surface, unlearning seems to be a promising solution for copyright protection:
    if a model violates the copyright of some content, we could attempt to unlearn
    said content. ^(It is conceivable that to resolve copyright violations via unlearning,
    provable and exact unlearning is necessary (and possibly sufficient); on the other
    hand, approximate unlearning, without guarantees and with the possibility of being
    hacked, is certainly insufficient and likely unnecessary.)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上看，遗忘似乎是解决版权保护的一个有前途的解决方案：如果模型侵犯了某些内容的版权，我们可以尝试遗忘该内容。 ^(可以想象，为了通过遗忘解决版权侵权问题，需要可证明并确切的遗忘是必要的（可能足够）；另一方面，近似遗忘，没有担保并存在被黑客攻击的可能性，显然是不足够的，也可能是不必要的。)
- en: 'In practice, however, there is a lot more nuance due to the questionable effectiveness
    of current unlearning methods and the [unclear legal landscape](https://arxiv.org/abs/2303.15715)
    at the intersection of AI and copyright. Since I am no legal expert (and clearly
    none of this section constitutes legal advice), we will mostly focus on asking
    questions. The central question seems to be: **is unlearning the right solution
    for copyright protection?**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，由于当前遗忘方法的有效性受到质疑，以及在人工智能和版权之间的[不明确的法律环境](https://arxiv.org/abs/2303.15715)，存在更多细微之处。由于我不是法律专家（很显然，本节的内容并不构成法律建议），我们将主要关注提出问题。中心问题似乎是：**遗忘是否是版权保护的正确解决方案？**
- en: 'Recall that the [fair use](https://en.wikipedia.org/wiki/Fair_use) doctrine
    ^(permits limited use of copyrighted material contigent on four factors: (1) purpose
    and character of the use (“transformativeness”), (2) the nature of the copyrighted
    work, (3) amount and substantiality of the use, and (4) the effect on material’s
    value. If the use of copyrighted content in a model qualifies as fair use, then
    unlearning such content from the model is unnecessary.)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 [公平使用](https://en.wikipedia.org/wiki/Fair_use) 原则 ^(允许有限使用受版权保护的材料，取决于四个因素：（1）使用的目的和性质（“变革性”），（2）受版权作品的性质，（3）使用的数量和实质性，以及（4）对材料价值的影响。如果在模型中使用的受版权内容符合公平使用标准，则取消学习此类内容无需必要。)
- en: Suppose a model is trained on some copyrighted content and is risking copyright
    violation, as in [New York Times v. OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html).
    Should OpenAI invest in (empirical) unlearning algorithms on ChatGPT? Or should
    they focus on the transformativeness axis of fair use and invest in deploying
    empirical *guardrails*, such as prompting, content moderation, and custom alignment
    to prevent the model from regurgitating training data? The latter seems to be
    what’s being implemented in practice.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个模型是在某些受版权保护的内容上进行训练，并且存在版权侵犯的风险，就像 [《纽约时报》诉 OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)
    中的情况。OpenAI 应该投资于 ChatGPT 上的（经验主义的）取消学习算法吗？或者他们应该专注于公平使用的变革性轴，并投资于部署经验 *防护栏*，例如提示、内容审核和定制对齐，以防止模型重复训练数据？后者似乎是实践中正在实施的方法。
- en: '**More broadly, there could also be economic solutions to copyright violation
    as alternatives to unlearning.** For example, model owners may provide an exact
    unlearning service (e.g. via periodic retraining) while also offering to indemnify
    model users for copyright infringement in the mean time, as seen in the case of
    OpenAI’s “[Copyright Shield](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)”.
    People are also starting to [explore](https://arxiv.org/abs/2404.13964) how one
    may price copyrighted data using Shapley values. In general, it is unclear right
    now how much of a role (if any) unlearning will play for resolving copyright related
    issues. Exact unlearning (extending to retrieval-based systems, see next section)
    does hold promises since deletion is clean and provable, but it seems that *legally
    binding* auditing procedures/mechanisms need to be first in place.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**更广义地说，也可以有经济上的解决方案来替代取消学习以解决版权侵犯问题。** 例如，模型所有者可以提供精确的取消学习服务（例如通过定期重新训练），同时在此期间为版权侵权向模型用户提供赔偿，就像在
    OpenAI 的 “[版权护盾](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)”案例中所见。人们也开始
    [探索](https://arxiv.org/abs/2404.13964) 如何使用 Shapley 值对受版权保护的数据定价。总的来说，目前还不清楚取消学习在解决与版权相关的问题中将发挥多大作用（如果有的话）。确切的取消学习（扩展到基于检索的系统，见下一节）确实很有希望，因为删除是干净且可证明的，但似乎需要首先建立
    *具有法律约束力的* 审计程序/机制。'
- en: 4.3\. Retrieval-based AI systems
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 基于检索的人工智能系统
- en: An obvious alternative to unlearning is to not learn at all. One way this could
    manifest for an LLM is that we take all content from the pre-training set that
    may receive unlearning requests (e.g., New York Times articles) and put them to
    an external data/vector store. Any questions relating to them will then be [RAG](https://arxiv.org/abs/2005.11401)’ed
    during inference, and any unlearning requests can be trivially satisfied by removing
    the data from the database. [Min et al.](https://arxiv.org/abs/2308.04430) demonstrates
    that this approach can be competitive to (though not quite matching) the trained
    baseline in terms of final perplexity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 不学习的一个明显替代方案是根本不学习。对于一个大型语言模型，这种情况可能表现为，我们从预训练集中获取可能会收到取消学习请求的所有内容（例如，《纽约时报》文章）并将其放入外部数据/向量存储。然后，在推断过程中，任何涉及这些内容的问题都将通过
    [RAG](https://arxiv.org/abs/2005.11401) 进行处理，任何取消学习请求都可以通过从数据库中删除数据轻松满足。[Min 等人](https://arxiv.org/abs/2308.04430)
    演示了这种方法在最终困惑度方面可以与（虽然不完全匹配）训练基准相竞争。
- en: 'Retrieval-based solutions are promising because of the increasing capabilities
    of the base models to reason in-context. However, there are few considerations
    before taking retrieval systems as the no-brainer solution to unlearning:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检索的解决方案是有希望的，因为基础模型在推理上下文中的能力不断增强。然而，在将检索系统作为取消学习的不费思考之举之前，需要考虑一些问题：
- en: '**Removing protected content from pre-training corpus can be a hard de-duplication
    problem.** Much like removing data contamination is [hard](https://lmsys.org/blog/2023-11-14-llm-decontaminator/),
    how can we be sure that paraphrases, quotations/citations, or other adaptations
    of the protected content are removed?'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从预训练语料库中移除受保护的内容可能是一个难以重复的问题。** 就像移除数据污染一样[困难](https://lmsys.org/blog/2023-11-14-llm-decontaminator/)，我们如何确保受保护内容的释义、引用/引证或其他改编已被移除？'
- en: '**What if the data to be unlearned can’t be retrieved?** Today we fine-tune
    many things into a model that aren’t documents or knowledge items; for example,
    it is unclear (yet) if things like as human preferences and desired behaviors
    (e.g. ability to write concisely) can be “retrieved” from a database.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如果要去学习的数据无法检索怎么办？** 今天我们对模型进行了许多微调，其中包括不是文档或知识项的东西；例如，目前还不清楚（但）像人类偏好和期望行为（例如写作简洁的能力）这样的事情是否可以从数据库中“检索”出来。'
- en: '**Dumping stuff in-context can open new attack surfaces.** Many RAG methods
    for LLMs work by putting related content in-context and ask the model to reason
    on them. Having the protected data in-context means they are now more susceptible
    to data extraction (simple prompting attacks may work just [fine](https://arxiv.org/abs/2402.17840)).'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将内容随机丢入环境中可能会打开新的攻击面。** 许多 RAG 方法用于 LLMs 的工作方式是将相关内容放入上下文中，并要求模型对其进行推理。将受保护的数据置于上下文中意味着它们现在更容易受到数据提取的影响（简单的提示攻击可能会起作用，[fine](https://arxiv.org/abs/2402.17840)）。'
- en: '**Utility gap between retrieval and training.** While there is evidence that
    retrieval-based solutions can be competitive, there is no general consensus that
    retrieval alone can replace fine-tune workloads; indeed, they can be [complementary](https://arxiv.org/abs/2401.08406).
    More broadly, what if the space of unlearnable data is too large such that if
    all of it goes to an external store, the base model wouldn’t be as useful?'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索和训练之间的效用差距。** 虽然有证据表明检索型解决方案可以竞争，但并没有普遍的共识认为仅靠检索就可以替代精细调整工作负载；事实上，它们可以是[互补的](https://arxiv.org/abs/2401.08406)。更广泛地说，如果不可学习数据的空间太大，以至于如果所有数据都进入外部存储，基本模型就不会那么有用，那么怎么办？'
- en: 4.4\. AI safety
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. AI 安全
- en: As models become more capable and are granted [agency](https://lilianweng.github.io/posts/2023-06-23-agent/),
    one concrete application domain for unlearning that is [gaining](https://www.gov.uk/government/topical-events/ai-safety-summit-2023)
    [traction](https://www.cnn.com/2024/04/26/tech/openai-altman-government-ai-safety-panel/index.html)
    is **AI safety**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型的能力越来越强，并被授予[代理](https://lilianweng.github.io/posts/2023-06-23-agent/)，一个具体的应用领域为去学习的[AI
    安全](https://www.gov.uk/government/topical-events/ai-safety-summit-2023)正在[获得](https://www.cnn.com/2024/04/26/tech/openai-altman-government-ai-safety-panel/index.html)
    [关注](https://www.cnn.com/2024/04/26/tech/openai-altman-government-ai-safety-panel/index.html)。
- en: 'Roughly speaking, safety concerns stem from a model’s *knowledge* (e.g., recipe
    of [napalm](https://youtu.be/zjkBMFhNj_g?t=2774)), *behaviors* (e.g., exhibiting
    [bias](https://arxiv.org/abs/2402.02680)), and *capabilities* (e.g., [hacking](https://arxiv.org/abs/2402.06664)
    websites). Examining current AI systems and extrapolating forward, one may imagine
    the following examples to apply unlearning and improve AI safety:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 大致而言，安全性问题源于模型的*知识*（例如，[凝固汽油的配方](https://youtu.be/zjkBMFhNj_g?t=2774)）、*行为*（例如，展示[偏见](https://arxiv.org/abs/2402.02680)）和*能力*（例如，[黑客攻击](https://arxiv.org/abs/2402.06664)网站）。检查当前的
    AI 系统并推断，可以想象以下示例应用于去学习并提高 AI 安全性：
- en: removing **hazardous knowledge**, as seen in the [WMDP](https://www.wmdp.ai/)
    benchmark;
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除**危险知识**，正如[WMDP](https://www.wmdp.ai/)基准所示；
- en: removing **model [poisons](https://arxiv.org/abs/2302.10149) and [backdoors](https://arxiv.org/abs/2401.05566)**,
    where models respond to adversarially planted input triggers;
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除**模型的[毒药](https://arxiv.org/abs/2302.10149)和[后门](https://arxiv.org/abs/2401.05566)**，即模型对敌对植入的输入触发器做出响应；
- en: removing **[manipulative](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)
    behaviors**, such as the ability to perform unethical persuasions or deception;
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除**[操纵性](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html)行为**，例如执行不道德的劝说或欺骗能力；
- en: removing **[bias](https://arxiv.org/abs/2402.02680) and [toxicity](https://arxiv.org/abs/2212.08061)**;
    or even
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除**[偏见](https://arxiv.org/abs/2402.02680)和[有毒性](https://arxiv.org/abs/2212.08061)**；甚至
- en: removing **[power-seeking](https://arxiv.org/abs/2206.13353) tendencies**.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除**[追求权力](https://arxiv.org/abs/2206.13353)倾向**。
- en: For safety-oriented applications, it is worth noting that unlearning should
    be treated as a post-training *risk mitigation and defense mechanism*, alongside
    existing tools like alignment fine-tuning and content filters. And as with any
    tool, we should view unlearning through its trade-offs in comparison to other
    tools in the toolbox (e.g., unlearning is more adaptive but more expensive than
    content filters), as opposed to brushing it off because of the potential lack
    of guarantees and efficacy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以安全为导向的应用程序，值得注意的是，去学习应该被视为一种后训练的*风险缓解和防御机制*，与现有的工具（如对齐微调和内容过滤器）并列。和任何工具一样，我们应该通过与工具箱中其他工具的权衡（例如，与内容过滤器相比，去学习更具适应性但更昂贵），而不是因为可能缺乏保证和效力而将其搁置。
- en: '* * *'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Acknowledgements**: The author would like to thank Aryaman Arora, Jiaao Chen,
    Irena Gao, John Hewitt, Shengyuan Hu, Peter Kairouz, Sanmi Koyejo, Xiang Lisa
    Li, Percy Liang, Eric Mitchell, Rylan Schaeffer, Yijia Shao, Chenglei Si, Pratiksha
    Thaker, Xindi Wu for helpful discussions and feedback before and during the drafting
    of this post. Any hot/bad takes are those of the author.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**致谢**：作者要感谢Aryaman Arora、Jiaao Chen、Irena Gao、John Hewitt、Shengyuan Hu、Peter
    Kairouz、Sanmi Koyejo、Xiang Lisa Li、Percy Liang、Eric Mitchell、Rylan Schaeffer、Yijia
    Shao、Chenglei Si、Pratiksha Thaker、Xindi Wu在起草本文之前和期间进行的有益讨论和反馈。任何火爆/糟糕的看法都属于作者本人。'
- en: '* * *'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Citation**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**引用**'
- en: 'If you find this post helpful, it can be cited as:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您觉得这篇文章有帮助，可以引用为：
- en: Liu, Ken Ziyu. (Apr 2024). Machine Unlearning in 2024\. Ken Ziyu Liu - Stanford
    Computer Science. [https://ai.stanford.edu/~kzliu/blog/unlearning](https://ai.stanford.edu/~kzliu/blog/unlearning).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 刘子瑜（2024年4月）。2024年机器去学习。刘子瑜 - 斯坦福大学计算机科学。[https://ai.stanford.edu/~kzliu/blog/unlearning](https://ai.stanford.edu/~kzliu/blog/unlearning)。
- en: Or
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE0]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '* * *'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](img/3a1f49bf66338957f45529edd7c2aa2e.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a1f49bf66338957f45529edd7c2aa2e.png)'
